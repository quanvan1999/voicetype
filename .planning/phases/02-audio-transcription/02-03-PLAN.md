---
phase: 02-audio-transcription
plan: 03
type: execute
wave: 2
depends_on: [02-01, 02-02]
files_modified:
  - LocalTranscript/LocalTranscript/Services/TranscriptionService.swift
  - LocalTranscript/LocalTranscript/Models/AppState.swift
  - LocalTranscript/LocalTranscript/Views/MenuBarView.swift
autonomous: false

must_haves:
  truths:
    - "User can trigger recording from menu bar"
    - "Recording start shows floating indicator and plays beep"
    - "Recording stop hides indicator, plays beep, and starts transcription"
    - "Vietnamese speech is transcribed using PhoWhisper model"
    - "Transcription result is displayed (for testing)"
  artifacts:
    - path: "LocalTranscript/LocalTranscript/Services/TranscriptionService.swift"
      provides: "Orchestrates recording and transcription flow"
      exports: ["TranscriptionService", "startRecording", "stopRecording", "TranscriptionState"]
  key_links:
    - from: "TranscriptionService"
      to: "AudioRecorder"
      via: "startRecording/stopRecording calls"
      pattern: "audioRecorder\\.(start|stop)Recording"
    - from: "TranscriptionService"
      to: "ModelManager.whisper"
      via: "whisper.transcribe call"
      pattern: "whisper\\.transcribe\\(audioFrames:"
    - from: "TranscriptionService"
      to: "FloatingIndicatorPanel"
      via: "show/hide on recording state"
      pattern: "floatingPanel\\.(orderFront|close)"
---

<objective>
Create TranscriptionService that orchestrates the complete recording-to-transcription flow, connecting AudioRecorder, ModelManager (Whisper), visual feedback, and audio feedback.

Purpose: This ties everything together - user triggers recording, audio is captured, then transcribed with PhoWhisper, and result is available. This plan adds a test button to verify the complete flow.
Output: TranscriptionService.swift with complete orchestration, test UI in MenuBarView
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-audio-transcription/02-RESEARCH.md
@.planning/phases/02-audio-transcription/02-01-SUMMARY.md
@.planning/phases/02-audio-transcription/02-02-SUMMARY.md

# From this phase
@LocalTranscript/LocalTranscript/Services/AudioRecorder.swift
@LocalTranscript/LocalTranscript/Services/ModelManager.swift
@LocalTranscript/LocalTranscript/Views/FloatingIndicatorPanel.swift
@LocalTranscript/LocalTranscript/Utilities/AudioFeedback.swift
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TranscriptionService with recording/transcription orchestration</name>
  <files>LocalTranscript/LocalTranscript/Services/TranscriptionService.swift</files>
  <action>
Create TranscriptionService that coordinates the full flow.

```swift
import Foundation
import SwiftWhisper
import AppKit

@Observable
class TranscriptionService {
    enum TranscriptionState {
        case idle
        case recording
        case transcribing
        case completed(String)
        case error(Error)
    }

    private(set) var state: TranscriptionState = .idle
    private(set) var lastTranscription: String = ""

    private let audioRecorder: AudioRecorder
    private let modelManager: ModelManager
    private var floatingPanel: FloatingIndicatorPanel?

    init(audioRecorder: AudioRecorder, modelManager: ModelManager) {
        self.audioRecorder = audioRecorder
        self.modelManager = modelManager
    }

    var isRecording: Bool {
        if case .recording = state { return true }
        return false
    }

    var isTranscribing: Bool {
        if case .transcribing = state { return true }
        return false
    }

    @MainActor
    func startRecording() async throws {
        guard case .idle = state else { return }

        // Ensure model is loaded (lazy loading)
        if !modelManager.isModelLoaded {
            try await modelManager.loadModel()
        }

        // Play start sound
        AudioFeedback.playStartSound()

        // Show floating indicator
        showFloatingIndicator()

        // Start recording
        try audioRecorder.startRecording()
        state = .recording
    }

    @MainActor
    func stopRecording() async {
        guard case .recording = state else { return }

        // Stop recording and get samples
        let samples = audioRecorder.stopRecording()

        // Hide floating indicator
        hideFloatingIndicator()

        // Play stop sound
        AudioFeedback.playStopSound()

        guard !samples.isEmpty else {
            state = .error(TranscriptionError.noAudioCaptured)
            return
        }

        // Transcribe
        state = .transcribing

        do {
            let text = try await transcribe(samples: samples)
            lastTranscription = text
            state = .completed(text)
        } catch {
            state = .error(error)
        }
    }

    /// Toggle recording (for button press)
    @MainActor
    func toggleRecording() async {
        if isRecording {
            await stopRecording()
        } else {
            do {
                try await startRecording()
            } catch {
                state = .error(error)
            }
        }
    }

    /// Reset to idle state (after showing result)
    func reset() {
        state = .idle
    }

    // MARK: - Private

    private func transcribe(samples: [Float]) async throws -> String {
        guard let whisper = modelManager.whisper else {
            throw TranscriptionError.modelNotLoaded
        }

        // Configure for Vietnamese
        let params = WhisperParams.default
        params.language = .vietnamese

        let segments = try await whisper.transcribe(audioFrames: samples, params: params)

        // Join segments with proper spacing
        return segments.map { $0.text.trimmingCharacters(in: .whitespaces) }
            .joined(separator: " ")
            .trimmingCharacters(in: .whitespaces)
    }

    @MainActor
    private func showFloatingIndicator() {
        if floatingPanel == nil {
            floatingPanel = FloatingIndicatorPanel(content: FloatingIndicator())
        }
        floatingPanel?.orderFront(nil)
    }

    @MainActor
    private func hideFloatingIndicator() {
        floatingPanel?.close()
        floatingPanel = nil
    }

    enum TranscriptionError: LocalizedError {
        case modelNotLoaded
        case noAudioCaptured

        var errorDescription: String? {
            switch self {
            case .modelNotLoaded:
                return "Whisper model not loaded"
            case .noAudioCaptured:
                return "No audio was captured"
            }
        }
    }
}
```

Key design decisions:
- State machine prevents invalid transitions
- Lazy model loading on first recording (as designed in Phase 1)
- FloatingIndicatorPanel managed internally
- Vietnamese language set explicitly in WhisperParams
- Segments joined with whitespace (PhoWhisper includes punctuation)
  </action>
  <verify>
Build succeeds: `cd /Users/sipher/Downloads/local-transcript-master/LocalTranscript && xcodebuild -scheme LocalTranscript -configuration Debug build 2>&1 | tail -20`
TranscriptionService.swift exists with startRecording, stopRecording, toggleRecording.
  </verify>
  <done>
TranscriptionService created with state machine, recording orchestration, and Whisper transcription.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate TranscriptionService into AppState and update isRecording</name>
  <files>LocalTranscript/LocalTranscript/Models/AppState.swift</files>
  <action>
Update AppState to use TranscriptionService and derive isRecording from it:

```swift
import SwiftUI

@Observable
class AppState {
    let permissionManager = PermissionManager()
    let modelManager = ModelManager()
    let launchManager = LaunchManager()
    let audioRecorder = AudioRecorder()

    // TranscriptionService coordinates everything
    lazy var transcriptionService: TranscriptionService = {
        TranscriptionService(audioRecorder: audioRecorder, modelManager: modelManager)
    }()

    // Derived from transcription service state
    var isRecording: Bool {
        transcriptionService.isRecording
    }
}
```

Note: Using `lazy var` because TranscriptionService needs audioRecorder and modelManager which are `let` constants initialized before.
  </action>
  <verify>
Build succeeds with AppState.transcriptionService.
isRecording is now computed from transcriptionService.
  </verify>
  <done>
AppState has transcriptionService with isRecording derived from service state.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add test recording button and result display to MenuBarView</name>
  <files>LocalTranscript/LocalTranscript/Views/MenuBarView.swift</files>
  <action>
Update MenuBarView to add recording controls and display transcription results.

Add to MenuBarView:
1. "Start Recording" / "Stop Recording" toggle button
2. "Last Transcription" section showing result
3. Status indicators for transcribing state

Example structure:
```swift
// Recording section
Section {
    Button {
        Task {
            await appState.transcriptionService.toggleRecording()
        }
    } label: {
        if appState.transcriptionService.isRecording {
            Label("Stop Recording", systemImage: "stop.circle.fill")
        } else if appState.transcriptionService.isTranscribing {
            Label("Transcribing...", systemImage: "waveform")
        } else {
            Label("Start Recording", systemImage: "record.circle")
        }
    }
    .disabled(appState.transcriptionService.isTranscribing)
}

// Result section (show if there's a transcription)
if !appState.transcriptionService.lastTranscription.isEmpty {
    Section("Last Transcription") {
        Text(appState.transcriptionService.lastTranscription)
            .lineLimit(3)

        Button("Copy") {
            NSPasteboard.general.clearContents()
            NSPasteboard.general.setString(
                appState.transcriptionService.lastTranscription,
                forType: .string
            )
        }
    }
}

// Error display
if case .error(let error) = appState.transcriptionService.state {
    Section {
        Text(error.localizedDescription)
            .foregroundStyle(.red)
    }
}
```

Keep existing sections (Model Status, Settings, Quit).
  </action>
  <verify>
Build succeeds with recording button in MenuBarView.
  </verify>
  <done>
MenuBarView has recording toggle button, transcription result display, and error handling.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete recording and transcription pipeline with Vietnamese speech recognition</what-built>
  <how-to-verify>
1. Ensure PhoWhisper model is downloaded:
   - Open app, go to Settings > Model
   - Click "Open Model Folder"
   - Place ggml-phowhisper-medium.bin in that folder (download from HuggingFace if not present)

2. Test recording flow:
   - Click menu bar icon (waveform)
   - Click "Start Recording" - should hear beep and see floating indicator
   - Speak some Vietnamese (e.g., "Xin chao, toi la mot nguoi Viet Nam")
   - Click "Stop Recording" - should hear beep, indicator disappears
   - Wait for "Transcribing..." to complete
   - Check "Last Transcription" section shows Vietnamese text

3. Verify visual feedback:
   - Menu bar icon changes to filled waveform with pulse during recording
   - Floating indicator visible at top of screen
   - Works in fullscreen app (open any app in fullscreen, start recording)

4. Test error handling:
   - Try recording without model (should show error)
   - Try very short recording (< 1 second)
  </how-to-verify>
  <resume-signal>Type "approved" if transcription works, or describe any issues observed</resume-signal>
</task>

</tasks>

<verification>
Full pipeline verification:
1. Recording starts with beep and floating indicator
2. Recording stops with beep, indicator hides
3. Vietnamese speech transcribed to text
4. Result displayed in menu bar dropdown
5. Copy button works
</verification>

<success_criteria>
- TranscriptionService orchestrates complete flow
- startRecording: loads model if needed, plays beep, shows indicator, starts capture
- stopRecording: stops capture, hides indicator, plays beep, transcribes
- Vietnamese speech transcribed using PhoWhisper
- Result displayed in MenuBarView with copy option
- Human verification confirms pipeline works end-to-end
</success_criteria>

<output>
After completion, create `.planning/phases/02-audio-transcription/02-03-SUMMARY.md`
</output>
