---
phase: 02-audio-transcription
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - LocalTranscript/LocalTranscript/Services/AudioRecorder.swift
autonomous: true

must_haves:
  truths:
    - "AudioRecorder can start recording from microphone"
    - "AudioRecorder accumulates audio samples during recording"
    - "AudioRecorder converts audio to 16kHz mono Float32 on stop"
    - "stopRecording returns [Float] samples ready for Whisper"
  artifacts:
    - path: "LocalTranscript/LocalTranscript/Services/AudioRecorder.swift"
      provides: "Audio capture with AVAudioEngine and format conversion"
      exports: ["AudioRecorder", "startRecording", "stopRecording", "isRecording"]
  key_links:
    - from: "AudioRecorder"
      to: "AVAudioEngine.inputNode"
      via: "installTap callback"
      pattern: "installTap\\(onBus:"
    - from: "AudioRecorder"
      to: "AVAudioConverter"
      via: "format conversion in processBuffer"
      pattern: "AVAudioConverter.*convert\\(to:"
---

<objective>
Create AudioRecorder service that captures microphone audio using AVAudioEngine and converts it to 16kHz mono Float32 format for Whisper transcription.

Purpose: This is the foundation of the recording pipeline - capturing audio in memory (not to file) and converting to the exact format Whisper expects.
Output: AudioRecorder.swift service with startRecording()/stopRecording() -> [Float]
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-audio-transcription/02-RESEARCH.md

# Existing code patterns
@LocalTranscript/LocalTranscript/Services/ModelManager.swift
@LocalTranscript/LocalTranscript/Models/AppState.swift
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AudioRecorder service with AVAudioEngine and buffer accumulation</name>
  <files>LocalTranscript/LocalTranscript/Services/AudioRecorder.swift</files>
  <action>
Create AudioRecorder.swift following the pattern from ModelManager (Observable class).

Key implementation details from research:
1. Use AVAudioEngine with inputNode.installTap for microphone capture
2. Query input format at runtime - do NOT hardcode sample rate (can be 16kHz AirPods, 44.1kHz built-in, 48kHz USB)
3. Create AVAudioConverter from inputFormat to targetFormat (16kHz mono Float32)
4. In tap callback, convert each buffer and append Float samples to array
5. On stopRecording, remove tap, stop engine, return accumulated samples

Target format (Whisper requirement):
```swift
let targetFormat = AVAudioFormat(
    commonFormat: .pcmFormatFloat32,
    sampleRate: 16000,
    channels: 1,
    interleaved: false
)!
```

Buffer conversion pattern (from research - handles variable-length input correctly):
```swift
private func processBuffer(_ inputBuffer: AVAudioPCMBuffer) {
    guard let converter = audioConverter else { return }

    let duration = Double(inputBuffer.frameLength) / inputBuffer.format.sampleRate
    let outputFrameCapacity = AVAudioFrameCount(16000.0 * duration)

    guard let outputBuffer = AVAudioPCMBuffer(
        pcmFormat: targetFormat,
        frameCapacity: outputFrameCapacity
    ) else { return }

    var error: NSError?
    var inputBufferConsumed = false

    converter.convert(to: outputBuffer, error: &error) { _, outStatus in
        if inputBufferConsumed {
            outStatus.pointee = .noDataNow
            return nil
        }
        inputBufferConsumed = true
        outStatus.pointee = .haveData
        return inputBuffer
    }

    if let channelData = outputBuffer.floatChannelData?[0] {
        let samples = Array(UnsafeBufferPointer(
            start: channelData,
            count: Int(outputBuffer.frameLength)
        ))
        accumulatedSamples.append(contentsOf: samples)
    }
}
```

Include:
- @Observable class AudioRecorder
- private var audioEngine: AVAudioEngine?
- private var audioConverter: AVAudioConverter?
- private var accumulatedSamples: [Float] = []
- private(set) var isRecording = false
- func startRecording() throws
- func stopRecording() -> [Float]
- enum AudioError: Error with converterCreationFailed case

Do NOT process heavily in tap callback - only append to array.
  </action>
  <verify>
Build succeeds: `cd /Users/sipher/Downloads/local-transcript-master/LocalTranscript && xcodebuild -scheme LocalTranscript -configuration Debug build 2>&1 | tail -20`
AudioRecorder.swift exists with AVAudioEngine and AVAudioConverter imports.
  </verify>
  <done>
AudioRecorder service created with startRecording/stopRecording methods, format conversion to 16kHz mono Float32.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add AudioRecorder to AppState and verify integration</name>
  <files>LocalTranscript/LocalTranscript/Models/AppState.swift</files>
  <action>
Add AudioRecorder instance to AppState, following the existing pattern:

```swift
@Observable
class AppState {
    var isRecording = false

    let permissionManager = PermissionManager()
    let modelManager = ModelManager()
    let launchManager = LaunchManager()
    let audioRecorder = AudioRecorder()  // Add this
}
```

Also add the new AudioRecorder.swift file to the Xcode project if needed (may auto-add).
  </action>
  <verify>
Build succeeds with AudioRecorder integrated into AppState.
Run: `cd /Users/sipher/Downloads/local-transcript-master/LocalTranscript && xcodebuild -scheme LocalTranscript -configuration Debug build 2>&1 | tail -20`
  </verify>
  <done>
AppState has audioRecorder property, app builds successfully.
  </done>
</task>

</tasks>

<verification>
Build the project and verify:
1. No compile errors
2. AudioRecorder class exists with correct API
3. AppState includes audioRecorder
</verification>

<success_criteria>
- AudioRecorder.swift created with AVAudioEngine + AVAudioConverter
- startRecording() installs tap and starts engine
- stopRecording() returns [Float] samples at 16kHz mono
- AppState includes audioRecorder instance
- Project builds without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-audio-transcription/02-01-SUMMARY.md`
</output>
